{"name":"Pengkun Liu's code for Implementation And Experimentation of \"Coherent Inference in Game Tree\" (Master thesis 2015)","tagline":"Fork of the code Pengkun Liu's Master thesis (Chalmers University), June 2015","body":"## Implementation And Experimentation of *Coherent Inference in Game Tree*\r\n> Implementation in C++, forked from [Charles-Lau-/thesis](https://github.com/Charles-Lau-/thesis).\r\n\r\n### Usage of this program\r\nYou can either use the [Makefile](https://github.com/Naereen/Pengkun--Master-Thesis--2015/blob/master/code/Makefile) or the [run](https://github.com/Naereen/Pengkun--Master-Thesis--2015/blob/master/code/run) script, or compile manually:\r\n\r\n```bash\r\n# Compile the program\r\ng++ -std=c++11 -o playGame *.cpp algorithm/*.cpp\r\n# Run it (and keep its output)\r\n./playGame | tee -a ./playGame.log\r\n```\r\n\r\nYou can have a look to the (long) [playGame.log](https://github.com/Naereen/Pengkun--Master-Thesis--2015/blob/master/code/playGame.log) file for an example of its output.\r\n\r\n### Limitation?\r\nThe implementation is generic enough to be able to play to higher dimension [Tic-Tac-Toe](https://en.wikipedia.org/wiki/Tic-Tac-Toe), not only the usual 3x3.\r\n\r\n### Example with a 4x4 Tic-Tac-Toe\r\nWhen the player **X** has to move, it examines a few possible moves, and compute the Gaussian value function learned for each position.\r\nFor instance, image that **X** has to play on this board state:\r\n```\r\nO O X O\r\n_ _ _ X\r\nO _ X X\r\n_ X X O\r\n```\r\n\r\nThen it will explore all the possible moves (6 in our example):\r\n```\r\nV value: Gaussian N(-0.224098, 0.862775)...\r\nO O X O\r\n_ _ _ X\r\nO _ X X\r\n_ X X O\r\n\r\n... 2 other moves\r\n\r\nV value: Gaussian N(0.939144, 0.521763)...  <-- The best one!\r\nO O X O\r\n_ _ X _\r\nO _ X X\r\n_ X X O\r\n\r\n\r\nV value: Gaussian N(-1.40412, 0.9321)...\r\nO O X O\r\n_ _ _ _\r\nO X X X\r\n_ X X O\r\n\r\n\r\nV value: Gaussian N(-0.658714, 0.932628)...\r\nO O X O\r\n_ _ _ _\r\nO _ X X\r\nX X X O\r\n```\r\n\r\nFinally, the Coherent Inference algorithm will sample from each of these Gaussian distribution (associated to each children of the current node).\r\nIt selects the move corresponding to the highest sample (usuall, from the more optimistic Gaussian value function V).\r\n\r\nIn the example showed above, ``N(0.93, 0.52)`` has a mean value of **0.93** (really higher than any other), so with high probability the sampling will select this action.\r\nHere it is the (only) one which directly brings **X** to a winning position:\r\n\r\n```\r\nO O X O\r\n_ _ X _\r\nO _ X X\r\n_ X X O\r\n```\r\n\r\nThis was only a very rough explanation of the algorithm.\r\nFor all the details, please refer to the initial paper, or [the slides or report for my project](https://bitbucket.org/lbesson/mva15-project-graph-reinforcement-learning/downloads/).\r\n\r\nAt the end, the *coherent Gaussian inference player* (**X**) **wins 72% of the time**, which for 4x4 Tic-Tac-Toe is an extremely satisfactory result!\r\n\r\n----\r\n\r\n### Overview of layout of the program\r\n#### ``Board.main()`` (in the [code](https://github.com/Naereen/Pengkun--Master-Thesis--2015/blob/master/code/) folder)\r\nEntrance of the program:\r\n\r\n- In this function, two computer players are initialized; and the ``Board`` is initialized.\r\n- Then a variable rounds is set, which means how many rounds are played between these two players.\r\n\r\n\r\n#### ``ComputerPlayer`` (in the [code](https://github.com/Naereen/Pengkun--Master-Thesis--2015/blob/master/code/) folder)\r\nModel a ComputerPlayer:\r\n\r\n- In this module, there are two functions, which are ``randomPlay`` and ``algorithmPlay`` respectively. These two functions will change the current board state. And they both return an identifier, which can let ``Board.main()`` determine whether the game should be continued or the game is winned by a player.\r\n- In ``randomPlay``, the player just plays the game randomly, while the player plays according to the algorithm in ``algorithmPlay``.\r\n- In ``algorithmPlay``, just three things are done: first, set up parameters like number of descents, the root of descent tree; second, call ``Ep.descent`` to do descent whose parameter the the root; third, determine which move is the best one.\r\n\r\n\r\n#### ``Ep`` (Expectation-Propagation) (in the [algorithm](https://github.com/Naereen/Pengkun--Master-Thesis--2015/blob/master/code/algorithm/) folder)\r\nCore of the expectation propagation algorithm:\r\n\r\n- Basically, in this module, the algorithm is implemented and all related message passing methods are implemented.\r\n- The method descent is the backbone and other message passing methods serve for it.\r\n- Additionally, all calculation related to Gaussian distribution are carried out in ``Distribution`` module.\r\n\r\n\r\n#### ``Distribution`` (in the [algorithm](https://github.com/Naereen/Pengkun--Master-Thesis--2015/blob/master/code/algorithm/) folder)\r\n- All dirty works of calculation about Gaussian distributions and correlated coefficient.\r\n- In this module, all messy distribution and correlated coefficient calcualtions are implemented.\r\n- Notice should be paid tothe Min/Max calculation of multiple dependent Gaussian distributions.\r\n\r\n#### ``Node`` (in the [algorithm](https://github.com/Naereen/Pengkun--Master-Thesis--2015/blob/master/code/algorithm/) folder)\r\nSupport build-up of ``Game`` tree:\r\n\r\n- It is an entity  module, which supports implementation of ``Game`` node in a ``Game`` tree.\r\n- In every node, it has both pointer to its parent and its children.\r\n- Also, binded values and distributions are its properties.\r\n- Notice: the ``getChild()`` function is varied if the ``Board`` is changed. For different game board, the strategy to get valid children nodes of a specific node is varied\r\n\r\n#### Other files?\r\n> The rest of the project are programs either easier to understand or less useful.\r\n> [See this other project for a more simple Reinforcement Learning agent for 3x3 Tic-Tac-Toe](http://naereen.github.io/Wesley-Tansey-RL-TicTacToe/).\r\n\r\n----\r\n\r\n### Authors and License\r\n- Forked and cleaned up by [Lilian Besson (Naereen)](https://github.com/Naereen), 28/12/2015, for [my MVA master project for the \"Reinforcement Learning\" course](https://bitbucket.org/lbesson/mva15-project-graph-reinforcement-learning/).\r\n- Created by [Pengkun Liu](https://github.com/Charles-Lau-/), April - June 2015,\r\n- Code released under the [MIT license](http://lbesson.mit-license.org).","google":"UA-38514290-17","note":"Don't delete this file! It's used internally to help with page regeneration."}